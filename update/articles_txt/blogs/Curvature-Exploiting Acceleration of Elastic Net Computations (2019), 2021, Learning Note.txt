CS690 Optimization Paper Notes 3
Due: March 12, 2021, 11:59 pm
Curvature-Exploiting Acceleration of Elastic Net Computations
Vien V. Mai and Mikael Johansson

1

Elastic Net Regression
minimize
xâˆˆRd

1
Î»2
||Ax âˆ’ b||22 + ||x||22 + Î»1 ||x||1
2n
2

â€¢ when Î»2 = 0, it is lasso regression.
â€¢ when Î»1 = 0, it is ridge regression.
In the real word, there are often a few dominant features, and some other features are strongly correlated
with these dominant features.

2
2.1

Related work
First-order method

Proximal Gradient Decent. Stochastic Proximal Gradient Decent is often used in large-scale problem, and the
time complexity is improved than using vanilla proximal method. In both cases, adding Nesterov momentum
to the optimization process could improve the time complexity.

2.2

Second-order method

Such as Newtons method that need to compute second-order derivatives during each iteration. It is robust
than first-order method, especially in non-linear and ill-conditioned cases. But this method require high
computational cost per iteration. There are some work that use an approximation approach, where they
estimating the Hessian matrix, but the time complexity is raised a lot.

3

Algorithm

Proximal gradient decent is the main type of method. In Elastic Net problem, the smooth part is the
objective function plus the ridge loss
1
Î»2
||Ax âˆ’ b||22 + ||x||22
2n
2
and the lasso loss as the undifferentiable part. In the approach of this paper, the optimizing process is a
method called ProxSVRG but with a second order method updating rule, i.e. a Newton-like method that will
normalize the gradients by the Hessian of the function before the proximal mapping step. The following is the
pseudo-code for the second-order ProxSVRG, which is an proximal stochastic gradient descent method, that
update the parameters with gradient at one data point. The global gradients are computed periodically, and
will be used to adjust stochastic gradients. Denote the learning rate as Î±, and a momentum-like coefficient Ï„ .

1

initialize(x0 );
for s = 0, 1, ...,PS do
n
âˆ‡f (xs ) = n1 i=1 âˆ‡fi (xs );
x0 = xs ;
z0 = x0 ;
for k=0, 1, ..., T do
randomly select j âˆˆ {0, 1, ..., n};
Ï„
1
xk + 1+Ï„
zk ;
yk = 1+Ï„
vk = âˆ‡fj (yk ) âˆ’ âˆ‡fj (xs ) + âˆ‡f (xs );
âˆ’1
xk+1 = proxH
vk );
Î±h (yk âˆ’ Î±H
1
gk+1 = Î± (yk âˆ’ xk+1 );
zk+1 = zk + Ï„ (yk âˆ’ zk ) âˆ’ ÂµÏ„ gk+1 ;
end
xs+1 = xT
end
return xS
âˆš
In order to reduce the noise and effect of damping, a mini-batch with size O( Îº) is often used for computing
gradient at each iteration (With Nesterov momentum, such size is under toleration), where Îº is the condition
number of the problem.

3.1

Approximation of Hessian

In the above algorithm, computing the true Hessian have time complexity O(nd2 ). The author proposed
a method that only require O(rd) time complexity, where r is the rank of the low-rank approximation of
the matrix A in the original objective function. By SVD, we have the best rank r approximation of matrix
Ar = Ur Î£r VrT . Then the Hessian is derived as
H = Vr (Î£2r + Î»2 I)VrT + (Ïƒr2 + Î»2 )(I âˆ’ Vr VrT )
where the authors state that the second term is used to capture information in the subspace orthogonal to
the column space of Vr . (Ïƒ is the singular value) Then the inverse of Hessian have the closed form
H âˆ’1 = Vr (Î£2r + Î»2 I)âˆ’1 VrT +

3.2

1
(I âˆ’ Vr VrT )
Ïƒr2 + Î»2

Condition Number and Time Complexity

The condition number is given as
P r(ÎºH

d
X
â‰¤ min(
i=1

P
rÏƒr + i>r Ïƒi
9
Ïƒi
,
+ d)) â‰¥
Î»2 (Ïƒi + Î»2 )
Î»2
10

Here Ïƒi are the singular value of the correlation matrix (AAT ), and d is the rank of correlation matrix.
The time complexity is given as
1
O(d(n + ÎºH ) log )

where the d here is the feature dimension, and  is the tolerant error to the optima.

2

